{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install Dependencies\n",
        "\n",
        "This cell installs the required packages such as nltk, torch, transformers, tensorflow, keras, numpy, matplotlib, and seaborn. These libraries provide a comprehensive toolkit for natural language processing, deep learning, and visualization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "VfvYsU-Ib1LU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 1: Install Dependencies\n",
        "!pip install nltk torch transformers tensorflow keras numpy matplotlib seaborn"
      ],
      "metadata": {
        "id": "4JgLgyikOciO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Import Necessary Libraries\n",
        "\n",
        "This cell imports all the essential libraries for text processing, deep learning, and data visualization. Libraries like nltk enable natural language processing, torch and tensorflow/keras are used for building neural networks, while matplotlib and seaborn allow for creating plots and visualizations.\n",
        "\n"
      ],
      "metadata": {
        "id": "XAfR6PSsb1LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 2: Import Necessary Libraries\n",
        "import nltk\n",
        "import torch\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "from transformers import pipeline"
      ],
      "metadata": {
        "id": "axT9kPAxOhT5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Sample Text Data\n",
        "\n",
        "This cell sets up a sample text string and downloads the necessary NLTK data packages (like 'punkt' and 'punkt_tab') for tokenization. These tokenizers break the text into words and punctuation needed for later analysis.\n"
      ],
      "metadata": {
        "id": "to9fs7PFb1LY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 3: Load Sample Text Data\n",
        "text = \"The quick brown fox jumps over the lazy dog. The fox is clever and quick.\"\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "2nmjcMpjOkzk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4: N-gram Model Explanation  \n",
        "\n",
        "This step implements a **Bigram Model**, a type of **N-gram model** where `n=2`. The purpose of this model is to analyze word sequences and determine the frequency of consecutive word pairs in the given text.  \n",
        "\n",
        "### How It Works  \n",
        "\n",
        "1. **Tokenization**  \n",
        "   - The input text is converted to lowercase to ensure uniformity.  \n",
        "   - It is then split into individual words (tokens) using `word_tokenize` from NLTK.  \n",
        "\n",
        "2. **Generating Bigrams**  \n",
        "   - The `ngrams` function from NLTK creates a list of word pairs (bigrams).  \n",
        "   - Each bigram consists of two consecutive words from the tokenized text.  \n",
        "\n",
        "3. **Counting Bigrams**  \n",
        "   - The `Counter` class from the `collections` module is used to count the frequency of each bigram.  \n",
        "   - The most common bigrams are displayed using the `most_common(5)` method.  \n",
        "\n",
        "### Theory and Applications  \n",
        "\n",
        "An **N-gram Model** estimates the probability of a word sequence based on observed word patterns. In the case of a bigram model, the probability of a word appearing is conditioned on the previous word.  \n",
        "\n",
        "**Applications**  \n",
        "- **Language Modeling:** Predicting the next word in a sentence.  \n",
        "- **Text Generation:** Constructing sentences based on learned word sequences.  \n",
        "- **Speech Recognition:** Improving accuracy by considering word sequence probabilities.  \n",
        "- **Machine Translation:** Enhancing translation quality by analyzing word pair patterns.  \n",
        "\n",
        "### Advantages  \n",
        "\n",
        "- **Simple and easy to implement** due to its straightforward structure.  \n",
        "- **Computationally efficient** compared to deep learning models, making it useful for quick text analysis tasks.  \n",
        "\n",
        "### Limitations  \n",
        "\n",
        "- **Limited context awareness** since it only considers the immediate previous word, making it ineffective for capturing long-range dependencies.  \n",
        "- **Data sparsity issue**, where unseen bigrams in the training data receive zero probability, affecting model accuracy.  \n",
        "\n",
        "### Additional References  \n",
        "\n",
        "- [NLTK n-grams Documentation](https://www.nltk.org/_modules/nltk/util.html)  \n",
        "- [Python Counter Class](https://docs.python.org/3/library/collections.html)  \n",
        "- [N-gram Models - Stanford NLP](https://web.stanford.edu/~jurafsky/slp3/3.pdf)  "
      ],
      "metadata": {
        "id": "UVUTWZ6wb1LZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 4: N-gram Model\n",
        "from nltk.tokenize import word_tokenize\n",
        "def generate_ngrams(text, n=2):\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    n_grams = list(ngrams(tokens, n))\n",
        "    return [\" \".join(gram) for gram in n_grams]\n",
        "\n",
        "ngram_counts = Counter(generate_ngrams(text, 2))\n",
        "print(\"Bigram counts:\", ngram_counts.most_common(5))"
      ],
      "metadata": {
        "id": "cw8HT2neOqwu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5: Simple RNN Model (PyTorch)  \n",
        "\n",
        "This step implements a **Simple Recurrent Neural Network (RNN)** using **PyTorch** for **character-level text generation**. The model learns patterns from input text and generates new sequences based on the learned character dependencies.  \n",
        "\n",
        "### How It Works  \n",
        "\n",
        "1. **Character-to-Index Mapping**  \n",
        "   - The input text is encoded into numerical form using a **character-to-index dictionary**.  \n",
        "   - Each character is assigned a unique index, allowing the model to process textual data in numerical format.  \n",
        "\n",
        "2. **One-Hot Encoding for Input Sequences**  \n",
        "   - The dataset is prepared using one-hot encoding, where each character is represented as a vector with a single \"1\" at the index corresponding to the character.  \n",
        "   - A **custom PyTorch Dataset class** is defined to generate input-target pairs from the text.  \n",
        "   - The **input sequence** consists of `seq_length` characters, and the **target** is the next character in the sequence.  \n",
        "\n",
        "3. **Defining the RNN Model**  \n",
        "   - The model consists of:  \n",
        "     - An **RNN layer** (`nn.RNN`) that processes input sequences and maintains hidden states.  \n",
        "     - A **fully connected layer** (`nn.Linear`) that maps the hidden state output to character predictions.  \n",
        "\n",
        "4. **Training the Model**  \n",
        "   - A **cross-entropy loss function** (`nn.CrossEntropyLoss`) is used to measure prediction accuracy.  \n",
        "   - The **Adam optimizer** updates model parameters.  \n",
        "   - The model is trained for **100 epochs**, adjusting weights to minimize loss.  \n",
        "\n",
        "5. **Text Generation**  \n",
        "   - A **text generation function** is implemented to predict the next character given an initial seed text.  \n",
        "   - The model iteratively generates new characters by selecting the most probable next character based on learned patterns.  \n",
        "\n",
        "### Theoretical Background  \n",
        "\n",
        "**Recurrent Neural Networks (RNNs)** are designed for **sequential data processing**, where each output depends on previous inputs. Unlike traditional feedforward networks, RNNs maintain a **hidden state** that retains past information, making them effective for modeling sequences.  \n",
        "\n",
        "However, **basic RNNs** face limitations such as **vanishing gradients**, where information from earlier time steps is lost as it propagates through the network. More advanced architectures like **LSTMs (Long Short-Term Memory) and GRUs (Gated Recurrent Units)** address this issue by introducing mechanisms for long-term memory retention.  \n",
        "\n",
        "### Applications  \n",
        "\n",
        "- **Text Generation:** Creating text sequences based on learned character dependencies.  \n",
        "- **Speech Recognition:** Converting spoken language into written text.  \n",
        "- **Machine Translation:** Translating text between languages.  \n",
        "- **Time Series Forecasting:** Predicting future values in time-dependent data.  \n",
        "\n",
        "### Advantages  \n",
        "\n",
        "- **Simple to implement** and computationally efficient for short sequences.  \n",
        "- **Effective for learning short-term dependencies in sequential data.**  \n",
        "\n",
        "### Limitations  \n",
        "\n",
        "- **Difficulty capturing long-term dependencies** due to vanishing gradients.  \n",
        "- **Limited performance compared to LSTM and GRU models**, which are better suited for longer sequences.  \n",
        "\n",
        "### Additional References  \n",
        "\n",
        "- [PyTorch RNN Documentation](https://pytorch.org/docs/stable/generated/torch.nn.RNN.html)  \n",
        "- [Understanding Recurrent Neural Networks](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
        "- [Sequence Modeling with RNNs](https://www.deeplearningbook.org/contents/rnn.html)  "
      ],
      "metadata": {
        "id": "dL8CBt13b1La"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 5: Simple RNN Model (PyTorch)\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from collections import defaultdict\n",
        "\n",
        "# Define the SimpleRNN model\n",
        "class SimpleRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(SimpleRNN, self).__init__()\n",
        "        self.rnn = nn.RNN(input_size, hidden_size, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out, _ = self.rnn(x)\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "# Preprocessing: Create a character-to-index mapping\n",
        "char_to_idx = defaultdict(lambda: len(char_to_idx))\n",
        "text_data = text\n",
        "encoded_text = [char_to_idx[char] for char in text_data]\n",
        "vocab_size = len(char_to_idx)\n",
        "\n",
        "# Prepare dataset\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, data, seq_length=3):\n",
        "        self.data = data\n",
        "        self.seq_length = seq_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data) - self.seq_length\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Change here: Create one-hot encoding for input sequence\n",
        "        input_seq = torch.zeros(self.seq_length, vocab_size)\n",
        "        for i, char_idx in enumerate(self.data[idx:idx + self.seq_length]):\n",
        "            input_seq[i, char_idx] = 1\n",
        "        return (\n",
        "            input_seq, # Remove the extra unsqueeze(0)\n",
        "            torch.tensor(self.data[idx + self.seq_length])\n",
        "        )\n",
        "\n",
        "# Training the SimpleRNN model\n",
        "hidden_size = 10\n",
        "dataset = TextDataset(encoded_text)\n",
        "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)\n",
        "rnn_model = SimpleRNN(vocab_size, hidden_size, vocab_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(rnn_model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(100):\n",
        "    for seq, target in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        output = rnn_model(seq)\n",
        "        loss = criterion(output, target)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Text Generation Function\n",
        "def generate_text(model, start_text, length=10):\n",
        "    model.eval()\n",
        "    input_seq = torch.tensor([char_to_idx[char] for char in start_text]).unsqueeze(0)\n",
        "    generated_text = start_text\n",
        "    for _ in range(length):\n",
        "        with torch.no_grad():\n",
        "            # Change here: Convert input sequence to one-hot encoding\n",
        "            input_seq_onehot = torch.zeros(1, input_seq.shape[1], vocab_size)\n",
        "            for i, char_idx in enumerate(input_seq[0]):\n",
        "                input_seq_onehot[0, i, char_idx.item()] = 1\n",
        "            output = model(input_seq_onehot)\n",
        "            predicted_idx = torch.argmax(output, dim=1).item()\n",
        "            predicted_char = list(char_to_idx.keys())[list(char_to_idx.values()).index(predicted_idx)]\n",
        "            generated_text += predicted_char\n",
        "            input_seq = torch.cat([input_seq[:, 1:], torch.tensor([[predicted_idx]])], dim=1)\n",
        "    return generated_text\n",
        "\n",
        "print(\"Generated text:\", generate_text(rnn_model, \"fox\", length=100))"
      ],
      "metadata": {
        "id": "BHQkJuRiPAST"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6: LSTM Model (TensorFlow/Keras)  \n",
        "\n",
        "This step implements an **LSTM-based neural network** using **TensorFlow/Keras** for **word-level text generation**. The model learns word sequences from input text and generates new sentences by predicting the next word based on previous words.  \n",
        "\n",
        "### How It Works  \n",
        "\n",
        "1. **Tokenization and Sequence Conversion**  \n",
        "   - The input text is tokenized using **Keras' `Tokenizer`**, which assigns a unique index to each word.  \n",
        "   - The text is converted into numerical sequences based on these word indices.  \n",
        "\n",
        "2. **Creating Input-Output Pairs**  \n",
        "   - A fixed **sequence length (`seq_length = 3`)** is defined.  \n",
        "   - Input sequences of `seq_length` words are extracted, with the next word as the target (`y`).  \n",
        "   - These pairs are stored in NumPy arrays (`X` and `y`) for training.  \n",
        "\n",
        "3. **Defining the LSTM Model**  \n",
        "   - **Embedding Layer (`Embedding`)**: Converts word indices into dense vector representations.  \n",
        "   - **LSTM Layers (`LSTM`)**: Two stacked LSTM layers process sequential input while retaining long-term dependencies.  \n",
        "     - The first LSTM layer returns sequences (`return_sequences=True`), allowing the second LSTM layer to process the entire sequence.  \n",
        "   - **Dense Output Layer (`Dense`)**: Uses **softmax activation** to output a probability distribution over possible next words.  \n",
        "\n",
        "4. **Compiling and Training the Model**  \n",
        "   - The model is compiled using **sparse categorical cross-entropy loss**, which is efficient for integer-labeled classification.  \n",
        "   - The **Adam optimizer** is used to update model parameters.  \n",
        "   - The model is trained for **100 epochs**, adjusting weights to improve accuracy.  \n",
        "\n",
        "5. **Text Generation**  \n",
        "   - A **text generation function** takes a seed phrase and predicts new words iteratively.  \n",
        "   - The function tokenizes the input, pads it to match the sequence length, and predicts the most likely next word using the trained model.  \n",
        "\n",
        "### Theoretical Background  \n",
        "\n",
        "**Long Short-Term Memory (LSTM)** networks are an advanced type of **Recurrent Neural Network (RNN)** specifically designed to address the **vanishing gradient problem** in traditional RNNs.  \n",
        "\n",
        "- **Vanishing Gradient Issue**: In standard RNNs, gradients diminish over long sequences, making it difficult to learn dependencies from earlier words.  \n",
        "- **LSTM Solution**: LSTMs introduce **gates (input, forget, and output gates)** that control how much past information is retained or forgotten, making them better at capturing **long-term dependencies** in sequential data.  \n",
        "\n",
        "### Applications  \n",
        "\n",
        "- **Text Generation:** Producing coherent text based on learned word patterns.  \n",
        "- **Speech Recognition:** Converting spoken language into written text.  \n",
        "- **Machine Translation:** Improving translation quality using context-aware sequences.  \n",
        "- **Sentiment Analysis:** Understanding sentiment in text by analyzing word sequences.  \n",
        "\n",
        "### Advantages  \n",
        "\n",
        "- **Better at learning long-term dependencies** compared to simple RNNs.  \n",
        "- **More stable training process**, reducing issues like exploding/vanishing gradients.  \n",
        "\n",
        "### Limitations  \n",
        "\n",
        "- **Higher computational cost** due to complex memory mechanisms.  \n",
        "- **Longer training times** compared to basic RNNs.  \n",
        "\n",
        "### Additional References  \n",
        "\n",
        "- [TensorFlow LSTM Documentation](https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM)  \n",
        "- [Understanding LSTMs – Colah’s Blog](https://colah.github.io/posts/2015-08-Understanding-LSTMs/)  \n",
        "- [Deep Learning for NLP – Stanford NLP](https://nlp.stanford.edu/~socherr/EMNLP2013_RNTN.pdf)  "
      ],
      "metadata": {
        "id": "-zKXM3vIb1Lc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 6: LSTM Model (TensorFlow/Keras)\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Sample text data\n",
        "text = \"The quick brown fox jumps over the lazy dog. The fox is clever and quick.\"\n",
        "\n",
        "# Tokenize text\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([text])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert text to sequences\n",
        "sequences = tokenizer.texts_to_sequences([text])[0]\n",
        "seq_length = 3  # Define input sequence length\n",
        "\n",
        "# Create input-output pairs\n",
        "X, y = [], []\n",
        "for i in range(len(sequences) - seq_length):\n",
        "    X.append(sequences[i:i+seq_length])\n",
        "    y.append(sequences[i+seq_length])\n",
        "\n",
        "X, y = np.array(X), np.array(y)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=64, input_length=seq_length),\n",
        "    LSTM(64, return_sequences=True),\n",
        "    LSTM(64),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(X, y, epochs=100, verbose=1)\n",
        "\n",
        "# Text generation function\n",
        "def generate_text(seed_text, next_words=5):\n",
        "    for _ in range(next_words):\n",
        "        tokenized = tokenizer.texts_to_sequences([seed_text])[0]\n",
        "        tokenized = pad_sequences([tokenized], maxlen=seq_length, padding='pre')\n",
        "        predicted_idx = np.argmax(model.predict(tokenized), axis=-1)[0]\n",
        "        for word, index in tokenizer.word_index.items():\n",
        "            if index == predicted_idx:\n",
        "                seed_text += \" \" + word\n",
        "                break\n",
        "    return seed_text\n",
        "\n",
        "# Generate text\n",
        "print(\"Generated text:\", generate_text(\"the jumping\"))"
      ],
      "metadata": {
        "id": "ATE5RvS1Pkt3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 7: Transformer Model (Hugging Face GPT-2)  \n",
        "\n",
        "This step demonstrates how to use a **pre-trained GPT-2 model** from Hugging Face’s `transformers` library for **text generation**. Unlike traditional RNN-based models, **GPT-2 leverages the Transformer architecture**, which enables **better context retention** and more **coherent text generation**.  \n",
        "\n",
        "### How It Works  \n",
        "\n",
        "1. **Loading the Pre-Trained Model and Tokenizer**  \n",
        "   - `GPT2Tokenizer` is used to tokenize input text and convert it into numerical format.  \n",
        "   - `GPT2LMHeadModel` is loaded with pre-trained GPT-2 weights to generate text.  \n",
        "\n",
        "2. **Text Generation Function**  \n",
        "   - The input text is **encoded** into token IDs using `tokenizer.encode()`.  \n",
        "   - The model generates new tokens **using self-attention mechanisms** while predicting the most probable next word.  \n",
        "   - The generated tokens are **decoded back to human-readable text** using `tokenizer.decode()`.  \n",
        "\n",
        "3. **Generating New Text**  \n",
        "   - The function takes a **prompt** (starting text) and generates a continuation of up to **50 tokens**.  \n",
        "   - The model ensures the generated text **flows naturally from the prompt**.  \n",
        "\n",
        "### Theoretical Background  \n",
        "\n",
        "**Transformers** (Vaswani et al., 2017) introduced the **self-attention mechanism**, which allows models to consider **the entire context** at once, rather than processing words sequentially (as in RNNs or LSTMs).  \n",
        "\n",
        "- **Self-Attention**: Each word in a sequence **attends to all other words**, helping the model understand **long-range dependencies**.  \n",
        "- **Decoder-Only Architecture**: GPT-2 is a decoder-based Transformer, meaning it generates text **autoregressively**, predicting one token at a time.  \n",
        "\n",
        "### Applications  \n",
        "\n",
        "- **Conversational AI (Chatbots, Virtual Assistants)**  \n",
        "- **Content Generation (Story, Blog, and Code Completion)**  \n",
        "- **Summarization and Paraphrasing**  \n",
        "- **Machine Translation**  \n",
        "\n",
        "### Advantages  \n",
        "\n",
        "- **Generates high-quality, coherent, and contextually relevant text.**  \n",
        "- **No need for training from scratch**—leverages large-scale pre-trained models.  \n",
        "- **Handles long-range dependencies** better than RNNs and LSTMs.  \n",
        "\n",
        "### Limitations  \n",
        "\n",
        "- **Computationally expensive**, requiring GPUs or TPUs for efficient inference.  \n",
        "- **Can generate biased or misleading text**, as it is trained on large-scale internet data.  \n",
        "- **May produce unexpected outputs** if prompt formatting or pre-processing is incorrect.  \n",
        "\n",
        "### Additional References  \n",
        "\n",
        "- [Hugging Face GPT-2 Documentation](https://huggingface.co/docs/transformers/model_doc/gpt2)  \n",
        "- [Original Transformer Paper (Vaswani et al., 2017)](https://arxiv.org/abs/1706.03762)  \n",
        "- [OpenAI’s GPT-2 Release Blog](https://openai.com/research/gpt-2)  "
      ],
      "metadata": {
        "id": "wNOF-WsRb1Ld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### Step 7: Transformer Model (Hugging Face GPT-2)\n",
        "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "def generate_text(prompt):\n",
        "    inputs = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
        "    outputs = model.generate(inputs, max_length=50, num_return_sequences=1)\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"GPT-2 Output:\", generate_text(\"The quick br\"))"
      ],
      "metadata": {
        "id": "rdGEiqtKPsVi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}